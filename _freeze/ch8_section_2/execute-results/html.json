{
  "hash": "eeb7964c714563c93ff582a004a7a346",
  "result": {
    "engine": "knitr",
    "markdown": "---\nfilters:\n  - naquiz\nformat:\n  html:\n    toc: true\n    toc-location: left\n    toc-title: \"In this section:\"\n---\n\n\n\n\n# Part 2 {.unnumbered #sec-mlpart02}\n\n## Background \n\nWe loaded and explored the histology imaging-derived data in [ML Part 1](ch8_section_1.qmd). Through this process we found:\n\n-   the data requires log-transformation\n\n-   there is little separation in malignant vs benign samples when clustering via principal components analysis\n\n-   using linear modeling, several imaging features show significant correlation with tumor status (benign vs malignant)\n\nGiven these findings, we have reason to believe that a machine learning algorithm may be successful in predicting malignant vs benign tumor status.\n\nBy contrast, if no differences are seen in PCA or simple testing of linear models, the chance of building a reliable classification model is much lower.\n\n## Aim\n\nTo train a classification model using the 'xgboost' algorithm (extreme gradient boosting), which is a class of algorithms that uses decision trees (similar to 'random forest').\n\nNote, our primary aim here is *not* to understand the individual features that contribute to the benign vs malignant classification, but simply to build a robust classification tool.\n\n## Load libraries\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(tidymodels)\nlibrary(xgboost)\n\ntheme_set(theme_minimal())\n```\n:::\n\n\n\n\n## Load data\n\nPreprocessed data from [ML Part 1](ch8_section_1.qmd) is stored in `data_processed/`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_wide_log <- read_xlsx('data_processed/tumor_data_log_wide.xlsx')\n```\n:::\n\n\n\n\n## Set seed\n\n'Reproducible randomness'. This ensures everyone using this tutorial gets the same results.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\n```\n:::\n\n\n\n\n## Format data\n\nTraining the ML model requires the outcome (malignant status) encoded as a factor. \n\n::: {.callout-tip title=\"Data types and file types\"}\nData types aren't preserved in Excel, csv or tsv format files. They are however preserved in .Rdata and .Rds files.\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_wide_log %>% select(1:5) %>%  str()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntibble [300 × 5] (S3: tbl_df/tbl/data.frame)\n $ donorid     : chr [1:300] \"d001\" \"d002\" \"d003\" \"d004\" ...\n $ status      : chr [1:300] \"malignant\" \"malignant\" \"malignant\" \"benign\" ...\n $ cell_size   : num [1:300] -0.769 0.469 0.68 0.669 1.113 ...\n $ cell_density: num [1:300] -0.1844 0.1517 0.6682 0.2399 -0.0523 ...\n $ nuclear_area: num [1:300] -0.0568 -0.1693 -0.2268 -0.3735 -0.3104 ...\n```\n\n\n:::\n:::\n\n\n\n\nRe-establish status as a factor:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_wide_log <- dat_wide_log %>% \n  mutate(status=factor(status))\n```\n:::\n\n\n\n\nDrop donorid from the training data. It encodes no useful infromation for training a model.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_final <- dat_wide_log %>% select(-c(donorid))\n```\n:::\n\n\n\n\n## Split test & train\n\nThe default is to split the data into training (75%) and test/hold-out (25%). We stratify by status, to ensure roughly equal numbers of benign and malignant samples in the test and training data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsplit_cl <- initial_split(data_final, strata = status)\n```\n:::\n\n\n\n\nExtract the test and training data to separate objects\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_cl <- training(split_cl)\n\ntest_cl  <- testing(split_cl)\n```\n:::\n\n\n\n\n\nCheck the balance of the status classes (malignant vs benign), remembering that ROC curves are more reliable when classes are balanced.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_cl %>% count(status)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  status        n\n  <fct>     <int>\n1 benign      104\n2 malignant   120\n```\n\n\n:::\n\n```{.r .cell-code}\ntest_cl  %>% count(status)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  status        n\n  <fct>     <int>\n1 benign       35\n2 malignant    41\n```\n\n\n:::\n:::\n\n\n\n\nEven splits!\n\n## Recipe\n\nThe data recipe stores the model parameters, and the data pre-processing steps. These steps will be performed on any data input to the ML training process.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_cl <- recipe(status ~ .  , data = train_cl ) %>% \n  step_zv(all_numeric_predictors()) %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_dummy(all_nominal_predictors()) \n```\n:::\n\n\n\n\nThe model algebra indicates we want to predict the `status` column (aka 'y' / 'outcome') using all available columns (denoted by `.` ). The data argument `train_cl` is provided to define the structure of the data (i.e., the colnames for all the features we have available).\n\nThe tidymodels package has a number of `step_()` functions that allow us to chain data transformation instructions together using the pipe `%>%`, similar to a dplyr 'chain'.\n\n`step_zv()` - step_zero_variance removes any features that have 0 variance, and therefore encode no information that can help to predict the outcome (which is 'status', in this case).\n\n`step_normalize()` performs a scaling normalization. First it centres the data each feature by subtracting the mean from every value, and then transforms the centred data into z scores (i.e., dividing each value by the standard deviation for that feature).\n\n`step_dummy()` - converts nominal (character) data such as sex, into 'dummy' numeric variables, to allow their use in the ML training data, which is strictly numeric.\n\nNote we have already log-transformed the data in our exploratory data analysis steps in `ml_pt1.qmd`. However, if we had not done this, we could use `step_log()` before `step_normalize()`.\n\n## Hyper-parameters\n\nHyper-parameters are pre-determined settings that govern how the model learns, and the model 'architecture' - akin to the depth and breadth of parameters that can be modified during training. \nDifferent model classes have different types of hyper-parameters. In this case for the xgboost algorithm, we pre-determine the number of decision trees that are 'grown' during the training steps, the depth ( = maximum number of branch-points) of those trees, and the 'learning rate' - which governs the magnitude of updates to the model during training.\n\nWe also determine the machine learning engine ('xgboost') and the mode ('classification') as opposed to 'regression'.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_params <- boost_tree(trees = 500, \n                           tree_depth = 20, \n                          learn_rate = 0.01) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"classification\")\n```\n:::\n\n\n\n\nOne of the great features about tidymodels is the simplicity of using different model classes. All that is required is to edit the `set_engine()` command to your model of choice! The pre-processing and evaluation steps remain unchanged, and this removes the need to learn a separate R library for each ML engine!\n\n## Workflow\n\nNow we package the data pre-processing recipe and the hyper-parameters into a workflow, ready for training. This workflow construct ensures that all steps - data pre-processing, model specification and tuning, are pre-defined in a single object.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_xgb <- workflow() %>%\n  # model outcome and features & pre-processing steps:\n  add_recipe(rec_cl) %>%  \n  #hyper-parameters, machine learning model type, and mode:\n  add_model(xgb_params) \n```\n:::\n\n\n\n\n::: {.callout-note title=\"Hyper-parameter tuning\"}\nEven though we are not tuning hyper-parameters in this simple workflow, the tuning process can be captured in the workflow object, as we will see later.\n:::\n\n::: {.callout-important title=\"Models within models?\"}\nIn this tutorial 'models' appear in two contexts\n\n1.  The model algebra (similar to a linear model equation), that specifies what our outcome is (y), and the predictor features we want to use. This is specified in the `recipe()` step.\n\n2.  The machine learning model (in this case xgboost), which is specified in the workflow using `add_model()`.\n:::\n\n## Train your model!\n\nWe provide the workflow and the input data. `fit()` performs the model training steps. This should take \\~30-60 seconds on a standard laptop.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_cl <- fit(wf_xgb, data = train_cl)\n```\n:::\n\n\n\n\n## Predict the test set!\n\nApply the newly trained model to predict the malignant status in the original `train_cl` training data, and the hold-out `test_cl` dataset. Here we set the type argument to 'prob', to generate _probabilities_ of each class, rather than discrete labels (0 or 1).\n\nThese probabilities will be used directly for plotting ROC and PR curves (below), and rounded for other diagnostic values.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_train <- predict(fit_cl, train_cl, type = 'prob') %>%\n  bind_cols(train_cl)\n\npred_test <- predict(fit_cl, test_cl, 'prob') %>%\n  bind_cols(test_cl)\n```\n:::\n\n\n\n\n## Evaluate\n\n### Round probabilities\n\nFirst a column of predicted class (discrete) values are generated by rounding the predicted probability of the malignant class\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_train <- pred_train |> \n  mutate(.pred_class=if_else(round(.pred_malignant)==1,\n                             'malignant',\n                             'benign')) |> \n  relocate(.pred_class, .before = status) |> \n  mutate(.pred_class=factor(.pred_class))\n\n\npred_test <- pred_test |> \n  mutate(.pred_class=if_else(round(.pred_malignant)==1,\n                             'malignant',\n                             'benign')) |> \n  relocate(.pred_class, .before = status) |> \n  mutate(.pred_class=factor(.pred_class))\n```\n:::\n\n\n\n\n::: {.callout-tip title=\"The rounding decision threshold\"}\nCreating PR and ROC curves requires varying the probability threshold at which a sample is labelled as TRUE (malignant) or FALSE (benign). What is the default decision threshold for the `round()` function?\n:::\n\n### Boxplot\n\nCompare the predicted probability of malignancy to the true sample labels\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_train |> \n  ggplot(aes(y=.pred_malignant, x=status)) + \n  geom_boxplot() + geom_jitter(width=0.2, height=0) +\n  ylab('Predicted probability of malignancy') +\n  xlab('True sample label') +\n  ggtitle('Training data')\n```\n\n::: {.cell-output-display}\n![](ch8_section_2_files/figure-html/unnamed-chunk-16-1.png){width=2000}\n:::\n\n```{.r .cell-code}\npred_test |> \n  ggplot(aes(y=.pred_malignant, x=status)) + \n  geom_boxplot() + geom_jitter(width=0.2, height=0) +\n  ylab('Predicted probability of malignancy') +\n  xlab('True sample label') +\n  ggtitle('Test data')\n```\n\n::: {.cell-output-display}\n![](ch8_section_2_files/figure-html/unnamed-chunk-16-2.png){width=2000}\n:::\n:::\n\n\n\n\n\n### Confusion matrix\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_train %>% conf_mat(truth = status, \n                        estimate = .pred_class, \n                        dnn = c('Predicted','Truth (TRAINING Data)')) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Truth (TRAINING Data)\nPredicted   benign malignant\n  benign       104         0\n  malignant      0       120\n```\n\n\n:::\n\n```{.r .cell-code}\npred_test  %>% conf_mat(truth=status, \n                        estimate = .pred_class, \n                        dnn = c('Predicted','Truth (TEST Data)'))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Truth (TEST Data)\nPredicted   benign malignant\n  benign        21         6\n  malignant     14        35\n```\n\n\n:::\n:::\n\n\n\n\nWhat do you notice about the performance of the model on the training, vs the test data?\n\n### Accuracy\n\nThe accuracy is the sum of the correct predictions divided by the total number of samples, where 1 = perfect accuracy.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_train %>% metrics(truth = status, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary             1\n2 kap      binary             1\n```\n\n\n:::\n\n```{.r .cell-code}\npred_test  %>% metrics(truth = status, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.737\n2 kap      binary         0.461\n```\n\n\n:::\n:::\n\n\n\n\n### Curves\n\nCreating ROC and PR curves require the predicted class probabilities rather than discrete labels. \n\nThe `roc_curve()` and `pr_curve()` functions from the `yardstick` package (part of the tidymodels stable) are very handy for calculating the true-positive and false-positive rates as the decision threshold decreases.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroc_tbl <- roc_curve(pred_test, truth = status, .pred_malignant, \n                     event_level = 'second')\n\npr_tbl <- pr_curve(pred_test, truth = status, .pred_malignant, \n                     event_level = 'second')\n```\n:::\n\n\n\n\nFor a quick look at performance, ROC and PR curves can be plotted using the `autoplot()` function (ggplot2 geoms can be added on for customization):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroc_tbl %>% autoplot() \n```\n\n::: {.cell-output-display}\n![](ch8_section_2_files/figure-html/unnamed-chunk-20-1.png){width=2000}\n:::\n\n```{.r .cell-code}\npr_tbl %>% autoplot() + geom_hline(yintercept = 0.5,lty=2)\n```\n\n::: {.cell-output-display}\n![](ch8_section_2_files/figure-html/unnamed-chunk-20-2.png){width=2000}\n:::\n:::\n\n\n\n\n### Area under the curve\n\n`yardstick` also contains `roc_auc()` and `pr_auc` to calculate the area under each curve type. Note that the 'event_level' argument is the category that we consider 'TRUE' (in this case 'malignant', which is the second level in the status factor).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# AUROCC\nroc_auc(data = pred_test, truth = status,  .pred_malignant, event_level = 'second') \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.804\n```\n\n\n:::\n\n```{.r .cell-code}\n# AUPRC\npr_auc( data = pred_test, truth = status, .pred_malignant, event_level = 'second') \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 pr_auc  binary         0.827\n```\n\n\n:::\n:::\n\n\n\n\n### Feature Gain\n\n'Gain' is a measure of the contribution of each feature for the accuracy of an xgboost model. \nUnderstanding the relative contribution of each feature is helpful if we want to create a smaller / lighter model using only the most important predictors, for example. The `vip` package ('variable importance plot') has a `vip()` function for creating the characteristic horizontal bar charts: \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Extract fitted xgboost model\nxgb_fit <- extract_fit_parsnip(fit_cl)$fit\n\n# Plot Gain  ('variable importance plot' - vip)\nvip::vip(xgb_fit, num_features = 20)\n```\n\n::: {.cell-output-display}\n![](ch8_section_2_files/figure-html/unnamed-chunk-22-1.png){width=2000}\n:::\n:::\n\n\n\n\nWhat do you notice about the most important features in this model, when compared to our original data exploration work in [ML Part 1](ch8_section_1.qmd) using `lm_test()`?\n\n## Save output\n\nLet's save the recipe `rec_cl`, and `pred_test` output from the model, to compare with a 'tuned' model in the next step.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsave(\n  #training & testing data:\n  train_cl, test_cl, \n     #recipe object:\n     rec_cl, \n     #predictions on test data:\n     pred_test, file='data_processed/ml_pt2_objects.Rda')\n```\n:::\n",
    "supporting": [
      "ch8_section_2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}