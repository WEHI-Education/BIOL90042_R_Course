{
  "hash": "43ad3c27b435d82bfd2466732a10ba6e",
  "result": {
    "engine": "knitr",
    "markdown": "---\nfilters:\n  - naquiz\nformat:\n  html:\n    toc: true\n    toc-location: left\n    toc-title: \"In this section:\"\n---\n\n\n\n\n\n# Part 3 {.unnumbered #sec-mlpart03}\n\n## Background\n\nUsing a dataset containing imaging-derived histology features, and tumor status (benign vs malignant), so far we have run [ML Part 1](ch8_section_1.qmd) to explore and transform the data, and [ML Part 2](ch8_section_2.qmd) to build an XGBoost model with pre-defined hyper-parameters.\n\n## Aim\n\nIn this final section, we will use the same input data to create an XGBoost classifier, but instead of pre-defining the model hyper-parameters, we will perform a 'grid search' to identify the combination of hyper-parameter values that produce the most accurate classifier.\n\nWe will then compare the tuned model to that created using pre-defined hyper-parameters in [ML Part 2](ch8_section_2.qmd).\n\n## Load libraries\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(tidymodels)\nlibrary(xgboost)\n\ntheme_set(theme_minimal())\n```\n:::\n\n\n\n\n\n## Load data\n\nFirst, we load the data created in [ML Part 2](ch8_section_2.qmd) \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload('data_processed/ml_pt2_objects.Rda')\n```\n:::\n\n\n\n\n\n## Set seed\n\nReproducible 'randomness'. This ensures everyone using this tutorial gets the same results.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n set.seed(165)\n# set.seed(42)\n #withr::with_seed(seed = 165)\n```\n:::\n\n\n\n\n\n## Cross training\n\nCross-fold validation requires the training data to be split into sub-sets of roughly equal size. The model is trained on N-1 sets and tested on he remaining 'unseen' data subset. Here we determine the folds for cross-validation during tuning, using `vfold_cv()`.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfolds <- vfold_cv(train_cl, v = 5, strata = status)\nfolds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#  5-fold cross-validation using stratification \n# A tibble: 5 × 2\n  splits           id   \n  <list>           <chr>\n1 <split [179/45]> Fold1\n2 <split [179/45]> Fold2\n3 <split [179/45]> Fold3\n4 <split [179/45]> Fold4\n5 <split [180/44]> Fold5\n```\n\n\n:::\n:::\n\n\n\n\n\n## Hyper-parameter tuning\n\nCompared with the single model with defined hyper-parameters in [ML Part 2](ch8_section_2.qmd), here we allow the tree_depth and learn_rate to be tuned, using a range of values across a 'grid'.\n\nIn the previous section we set the exact parameters for the `boost_tree()` function. Here we allow the tree depth and the learning rate to be tuned for optimal performance, by running multiple models across a range of tree depth and learning rate parameters. Note that these arguments are now set to `tune()`.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_spec_tune_cl <- boost_tree(trees = 500, \n                               tree_depth = tune(), \n                               learn_rate = tune()) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"classification\")\n```\n:::\n\n\n\n\n\n## Tuning workflow\n\nWe construct a workflow where the xgboost model now includes the original recipe from [ML Part 2](ch8_section_2.qmd), and the tuning steps defined above.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_xgb_tune  <- workflow() |> \n  # model outcome and features & pre-processing steps:\n  add_recipe(rec_cl) |> \n  #hyper-parameters, machine learning model type, and mode:\n  add_model(xgb_spec_tune_cl)\n```\n:::\n\n\n\n\n\nRunning the `tune_grid()` command iterates through the hyper-parameter values in `grid`, and determines the optimal combinations via cross-fold validation (using the `folds` object). We also set the evaluation metrics as area under the ROC curve, and mean log loss. Note that log-loss for classification tasks, is similar to RMSE for regression. It measures the accuracy by penalizing false predictions. The lower the log-loss the better.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid <- grid_regular(tree_depth(), learn_rate(), levels = 5)\n\n\nxgb_tune_res <- tune_grid(\n  wf_xgb_tune,\n  resamples = folds,\n  grid = grid,\n  metrics = metric_set(roc_auc, mn_log_loss),\n  control = control_grid(save_pred = TRUE)\n)\n```\n:::\n\n\n\n\n\nNB this step can take 30-60 seconds on a standard laptop with the current data.\n\n# Grid search results\n\nWe can generate a plot to inspect the performance metrics for each combination of hyper-parameters. The models with the highest mean values have the best performance.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(xgb_tune_res) |> \n  ggplot(aes(x = factor(tree_depth), y = mean)) + \n  geom_point(aes(size = log10(learn_rate)), pch = 1) + \n  facet_wrap(~ .metric) + \n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](ch8_section_3_files/figure-html/unnamed-chunk-8-1.png){width=2000}\n:::\n:::\n\n\n\n\n\nThe predictions for every cross-fold iteration across the tuning grid, can be accessed via `collect_predictions(xgb_tune_res, summarize = F)`.\n\nIf considering only the AUROCC metric, we can create a tile plot to identify the optimal hyper-parameters.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(xgb_tune_res, metric = 'roc_auc', n = Inf) |>\n  mutate(log_learnrate = log10(learn_rate)) |>\n  ggplot(aes(\n    x = factor(log_learnrate),\n    y = factor(tree_depth),\n    fill = mean\n  ))  +\n  geom_tile(col = 'white') \n```\n\n::: {.cell-output-display}\n![](ch8_section_3_files/figure-html/unnamed-chunk-9-1.png){width=2000}\n:::\n:::\n\n\n\n\n\n### Select best hyper-parameters\n\nWe can see from the plots that models with a learning rate of 0.1 perform the best. There is little difference by tree depth however. To extract the combination of hyper-parameters that performs the best, use `select_best()`. Note we can only use one metric to determine the best model!\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_params <- select_best(xgb_tune_res, metric =\"roc_auc\" )\nbest_params\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  tree_depth learn_rate .config              \n       <int>      <dbl> <chr>                \n1         11        0.1 Preprocessor1_Model24\n```\n\n\n:::\n:::\n\n\n\n\n\nConfirm that the best model has been selected, by printing all results and sorting on descending mean:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(xgb_tune_res) |> arrange(desc(mean))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 50 × 8\n   tree_depth   learn_rate .metric .estimator  mean     n std_err .config       \n        <int>        <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>         \n 1         11 0.1          roc_auc binary     0.839     5 0.00737 Preprocessor1…\n 2         15 0.1          roc_auc binary     0.839     5 0.00737 Preprocessor1…\n 3          8 0.1          roc_auc binary     0.838     5 0.00734 Preprocessor1…\n 4          4 0.1          roc_auc binary     0.837     5 0.0109  Preprocessor1…\n 5          1 0.1          roc_auc binary     0.837     5 0.0190  Preprocessor1…\n 6          1 0.000562     roc_auc binary     0.787     5 0.0196  Preprocessor1…\n 7          4 0.000562     roc_auc binary     0.757     5 0.0268  Preprocessor1…\n 8          4 0.0000000178 roc_auc binary     0.755     5 0.0293  Preprocessor1…\n 9          4 0.00000316   roc_auc binary     0.752     5 0.0250  Preprocessor1…\n10          8 0.0000000178 roc_auc binary     0.747     5 0.0339  Preprocessor1…\n# ℹ 40 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\n### Finalize workflow\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_wf <- finalize_workflow(wf_xgb_tune, best_params)\n```\n:::\n\n\n\n\n\n## Train your model!\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_fit <- fit(final_wf, data = train_cl)\n```\n:::\n\n\n\n\n\n## Predict the test set!\n\nApply the model trained using optimal hyper-parameters, to predict the malignant status in the hold-out `test_cl` dataset.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_tuned <- predict(final_fit, test_cl, type = \"prob\") %>%\n  bind_cols(test_cl)\n```\n:::\n\n\n\n\n\n# Compare tuning effects\n\n## Evaluate\n\nHere we generate performance curves and metrics for ROC and PR, for both the original and the tuned models.\n\n### Original hyper-parameters\n\nAssign the original model results to `pred_orig`, then extract the ROC and PR data for the original model:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_orig <- pred_test\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nroc_orig <- roc_curve(pred_orig, truth = status, .pred_malignant, \n                      event_level = \"second\") %>%\n  mutate(model = \"Original\") \n\n\npr_orig <- pr_curve(pred_orig, truth = status, .pred_malignant, \n                    event_level = \"second\") %>%\n  mutate(model = \"Original\") \n```\n:::\n\n\n\n\n\n### Tuned hyper-parameters\n\nExtract the ROC and PR results for the tuned model\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroc_tuned <- roc_curve(pred_tuned, truth = status, .pred_malignant, \n                       event_level = \"second\") %>%\n  mutate(model = \"Tuned\") \n\n\npr_tuned <- pr_curve(pred_tuned, truth = status, .pred_malignant, \n                     event_level = \"second\") %>%\n  mutate(model = \"Tuned\") \n```\n:::\n\n\n\n\n\n## Statistics\n\n### Original hyper-parameters\n\nCalculate the AUC for the original model:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naurocc_orig <- roc_auc(pred_orig,  truth = status, .pred_malignant, \n                       event_level = \"second\") |> \n  pull(.estimate) |> round(3)\n\nauprc_orig <- pr_auc(pred_orig,  truth = status, .pred_malignant, \n                     event_level = \"second\") |> \n  pull(.estimate) |> round(3)\n```\n:::\n\n\n\n\n\n### Tuned parameters\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naurocc_tuned <- roc_auc(pred_tuned, truth = status, .pred_malignant,\n                        event_level = \"second\") |> \n  pull(.estimate) |> round(3)\n\n\nauprc_tuned <- pr_auc(pred_tuned, truth = status, .pred_malignant, \n                      event_level = \"second\") |> \n  pull(.estimate) |> round(3)\n```\n:::\n\n\n\n\n\n## Plots\n\nHere we create ROC and PR curves using ggplot, to directly compare the performance of the original vs the tuned classifiers. (We sort the data by sensitivity, and precision respectively, to avoid unwanted extra lines in the `geom_step()` geom)\n\n### ROC\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroc_origVtuned <- bind_rows(\n  roc_orig  |> arrange(sensitivity), \n  roc_tuned |> arrange(sensitivity) ) \n  \nroc_origVtuned |> ggplot(aes(x = 1-specificity, y = sensitivity)) +\n  geom_step(aes(color=model), lwd=2) +\n  geom_abline(linetype = \"dashed\", color = \"grey\") +\n  annotate(\"text\", x = 0.6, y = 0.2, \n           label = paste0(\"AUROC (orig): \",  aurocc_orig) ) +\n  annotate(\"text\", x = 0.6, y = 0.1, \n           label = paste0(\"AUROC (tuned): \", aurocc_tuned)) +\n  labs(title = \"ROC Curve Comparison\",\n       x = \"False Positive Rate\",\n       y = \"True Positive Rate\",\n       color = \"Model\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](ch8_section_3_files/figure-html/unnamed-chunk-20-1.png){width=2000}\n:::\n:::\n\n\n\n\n\n### PR\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npr_origVtuned <- bind_rows(\n  pr_orig  |> arrange(desc(precision)),\n  pr_tuned |> arrange(desc(precision)))\n  \npr_origVtuned |> \n  ggplot(aes(x = recall, y = precision)) +\n  geom_step(aes(color=model), lwd=2) +\n  geom_hline(linetype = \"dashed\", color = \"grey\",yintercept = 0.5) +\n  annotate(\"text\", x = 0.6, y = 0.2, \n           label = paste0(\"AUROC (orig): \",  auprc_orig) ) +\n  annotate(\"text\", x = 0.6, y = 0.1, \n           label = paste0(\"AUROC (tuned): \", auprc_tuned)) +\n  labs(title = \"PR Curve Comparison\",\n       x = \"Recall\",\n       y = \"Precision\",\n       color = \"Model\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](ch8_section_3_files/figure-html/unnamed-chunk-21-1.png){width=2000}\n:::\n:::\n\n\n\n\n\n# RESULTS\n\nBy tuning the model hyper-parameters, we identified an optimum that allowed us to build a classifier that has slightly better accuracy than our original model!\n",
    "supporting": [
      "ch8_section_3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}