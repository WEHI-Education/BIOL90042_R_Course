{
  "hash": "59fe842a6592b1206bfa7534b8838291",
  "result": {
    "engine": "knitr",
    "markdown": "---\nfilters:\n  - naquiz\nformat:\n  html:\n    toc: true\n    toc-location: left\n    toc-title: \"In this chapter:\"\n---\n\n\n\n\n# Statistics in R Primer  {.unnumbered}\n\nIn the second half of this book we will use statistics in R for exploratory data analysis, bioinformatics and machine learning.\nAt this juncture its helpful to distinguish three domains in which we'll apply statistics\n\n1.  general linear modelling in tidyverse workflows\n\n2.  linear modelling for differential gene expression testing\n\n3.  machine learning for sample classification\n\n# Linear modelling in the tidyverse\n\nLinear models are very useful for testing for significant associations between variables, or significant differences between groups.\nThey have the advantage of being able to correct for the effects of other variables, and thereby remove unwanted sources of variation that could otherwise reduce our ability to discover true relationships.\n\nLinear modeling is essentially a two-step process: fitting a straight line through the data (making a linear model of the relationship between x and y), and then testing whether the slope of that line is 'greater than would be expected by chance' via a hypothesis test.\nIn practice the slope and variance around the fitted line, and the p value for the hypothesis test, are estimated using a single function and reported in the same summary table.\n\nIn the R tidyverse, plotting data is a central part of exploratory data analysis, with ggplot2 as the foundational package.\nWe will reuse the data from [Chapter 5](chapter_5.qmd) to investigate linear modeling in the tidyverse.\n\nFirst we load the `.Rdata` file to restore the environment objects (data frames) we created:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload('data_processed/mz_cleaned_joined.Rdata')\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidyrstats)\n```\n:::\n\n\n\n\n## Testing differences between groups (discrete x)\n\nLong format data works seamlessly with `ggplot()`, as follows:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmz_expr_rep_summed\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,359 × 4\n   id_num group     gene  expression_level\n    <dbl> <chr>     <chr>            <dbl>\n 1      1 treatment Th               6204.\n 2      1 treatment Prlh             1045.\n 3      1 treatment Hprt1            3029.\n 4      2 treatment Th              10738.\n 5      2 treatment Prlh             1568.\n 6      2 treatment Hprt1            3669.\n 7      3 treatment Th              12918.\n 8      3 treatment Prlh              813.\n 9      3 treatment Hprt1            4297.\n10      4 treatment Th              10892.\n# ℹ 1,349 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nmz_expr_rep_summed %>% \n  ggplot(aes(x=group,y=expression_level)) +\n  geom_boxplot() + \n  facet_wrap(~gene, scales='free')\n```\n\n::: {.cell-output-display}\n![](stats_primer_files/figure-html/unnamed-chunk-3-1.png){width=2000}\n:::\n:::\n\n\n\n\nOften in an exploratory data analysis, we want to quickly test for statistical significant differences in the data we have just plotted.\n\nHere, we want to test the differences in mean gene expression by treatment, for each gene.\nThe `lm_test()` function from tidyrstats allows us to do this using the exact same data format required for ggplot.\n\n`lm_test()` is simply a wrapper for some fairly awful code that we would otherwise have to use here (in the form `data %>% do(broom::tidy(lm(y~x, data = .)`).\n\nLet's see how this works below, by grouping the data so that we have a separate test for each gene:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmz_expr_rep_summed %>% \n  group_by(gene) %>% \n  lm_test(expression_level ~ group)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nResults for linear model: expression_level ~ group\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  gene  term           estimate std.error statistic   p.value\n  <chr> <chr>             <dbl>     <dbl>     <dbl>     <dbl>\n1 Hprt1 intercept        2951.       43.3     68.1  2.07e-239\n2 Prlh  intercept         906.       25.0     36.3  8.43e-136\n3 Th    intercept        9192.      333.      27.6  6.94e- 99\n4 Prlh  grouptreatment    218.       28.7      7.60 1.72e- 13\n5 Th    grouptreatment   1716.      382.       4.49 9.12e-  6\n6 Hprt1 grouptreatment     72.4      49.7      1.46 1.46e-  1\n```\n\n\n:::\n:::\n\n\n\n\nWe can ignore the intercept term rows, and inspect the p value column for significant values (p\\<0.05).\n\nWe see that Prlh and Th show significant differences in expression between treatment and control groups.\nIn the model formulation `expression_level ~ group`, the control group mean is the intercept, and the treatment group is the tested term.\n\n## Testing differences relative to 0 (discrete x)\n\nIf we use `lm_test(expression_level ~ 0 + group)` we will get the mean expression per group as the `estimate`, and the difference in each group mean from 0, is tested.\nHowever, the differences between the mean gene expression levels in each group, are not directly tested:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmz_expr_rep_summed %>% \n  group_by(gene) %>% \n  lm_test(expression_level ~ 0 + group)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nResults for linear model: expression_level ~ 0 + group\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  gene  term           estimate std.error statistic   p.value\n  <chr> <chr>             <dbl>     <dbl>     <dbl>     <dbl>\n1 Hprt1 grouptreatment    3023.      24.4     124.  0        \n2 Prlh  grouptreatment    1124.      14.1      79.9 2.01e-268\n3 Hprt1 groupcontrol      2951.      43.3      68.1 2.07e-239\n4 Th    grouptreatment   10908.     188.       58.2 9.99e-212\n5 Prlh  groupcontrol       906.      25.0      36.3 8.43e-136\n6 Th    groupcontrol      9192.     333.       27.6 6.94e- 99\n```\n\n\n:::\n:::\n\n\n\n\nNote that the p values are wildly significant (because every gene in each group has vastly higher expression than 0).\nHowever, we cannot easily conclude whether the treatment is having a significant effect on gene expression compared to control.\n\nTo answer this question requires a second round of specific pair-wise 'contrast testing', which is most relevant for RNA sequencing analysis, covered in later chapters.\n\n## Testing associations between variables (continuous x)\n\nWe can also test for associations between a continuous x and a continuous y variable, using lm_test().\nTo plot expression of different genes in a scatter plot, values for each gene must reside in separate columns\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n mz_expr_rep_summed_wide %>% \n   ggplot( aes(x = Th, y = Prlh)) +\n    geom_point() +\n   geom_smooth(method='lm')\n```\n\n::: {.cell-output-display}\n![](stats_primer_files/figure-html/unnamed-chunk-6-1.png){width=2000}\n:::\n:::\n\n\n\n\nThe same tibble can be fed directly into `lm_test()` to test the significance of the Prlh \\~ Th gene expression association described by the `geom_smooth()` line above.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmz_expr_rep_summed_wide %>% \n  lm_test(Prlh ~ Th)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nResults for linear model: Prlh ~ Th\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 intercept 1035.       40.6        25.5   2.02e-89\n2 Th           0.00348   0.00366     0.948 3.43e- 1\n```\n\n\n:::\n:::\n\n\n\n\nThe y intercept is \\~1000, and for each incremental unit of increase in expression of Th, Prlh is predicted to increase by a further 0.003 units.\nThis association is not significantly stronger than would be expected by chance, as the T statistic is 0.9, and the p value for the T test is 0.34.\n\n## Correcting for covariates\n\nThus far we have tested relationships between a single covariate and a response (y) variable.\n\nOften we want to ensure that other possible sources of variation are minimized so that we can better detect true differences, aka test our 'main effect of interest'.\nSay we want to understand whether the Mousezempic dosage affects gene expression *regardless of mouse strain*.\nThat is, we want to subtract the mean effect of any strain-specific differences.\n\nFirst to inspect any differences in gene expression between strains:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmz_joined_data_longer %>%\n  ggplot(aes(y = expression, x = mouse_strain)) + geom_boxplot()  +\n  geom_jitter(size = 0.2, height = 0, width = 0.2) +\n  facet_wrap( ~ gene, scales = 'free_y')\n```\n\n::: {.cell-output-display}\n![](stats_primer_files/figure-html/unnamed-chunk-8-1.png){width=2000}\n:::\n:::\n\n\n\n\nWe can see there are modest (likely non-significant) differences in mean gene expression between strains.\n\nFor the gene *Prlh* we can plot the expression \\~ dose relationship colouring points by mouse strain:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmz_joined_data_longer %>%\n  filter(gene == 'Prlh') %>%\n  ggplot(aes(x = drug_dose_g, y = expression)) +\n  geom_point(aes(col = mouse_strain))  +\n  geom_smooth(method = 'lm') + \n  ggtitle('Prlh gene')\n```\n\n::: {.cell-output-display}\n![](stats_primer_files/figure-html/unnamed-chunk-9-1.png){width=2000}\n:::\n:::\n\n\n\n\nTo test the association between drug dose and gene expression without correcting for mouse strain (i.e., testing the slope of the fitted blue line above):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmz_joined_data_longer %>% \n  filter(gene=='Prlh') %>% \n  lm_test(expression ~ drug_dose_g)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nResults for linear model: expression ~ drug_dose_g\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 intercept       967.      219.     4.41  0.0000140\n2 drug_dose_g   76201.   108876.     0.700 0.484    \n```\n\n\n:::\n:::\n\n\n\n\nTo estimate and then remove any gene expression differences between strains, before testing the drug treatment association, we can use the model `expression ~ drug_dose_g + strain`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmz_joined_data_longer %>% \n  filter(gene=='Prlh') %>% \n  lm_test(expression ~ drug_dose_g + mouse_strain)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nResults for linear model: expression ~ drug_dose_g + mouse_strain\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  term                estimate std.error statistic p.value\n  <chr>                  <dbl>     <dbl>     <dbl>   <dbl>\n1 intercept             1145.      452.     2.53    0.0117\n2 mouse_strainCD-1       -20.4      43.0   -0.474   0.636 \n3 mouse_strainBlack 6     12.3      65.0    0.189   0.850 \n4 drug_dose_g          -9768.   229977.    -0.0425  0.966 \n```\n\n\n:::\n:::\n\n\n\n\nNotice that the p value for the drug_dose_g (our main effect of interest), is 0.48 before correcting for mouse strain, and 0.96 after correcting.\n\nThis indicates that there is *less* effect of drug dose on gene expression than we would otherwise have reported.\nPut another way, the slightly stronger association between drug dose and *Prlh* expression using the model `expression ~ drug_dose_g`, is simply the result of underlying differences in gene expression between strains!\n\n::: {.callout-tip title=\"Marginal vs Conditional effects\"}\nThe estimate (slope) for a simple linear model with a single x variable, is called the 'marginal effect'.\n\nThe estimates for a multiple linear regression model (where effects for each covariate are calculated after setting the mean effect of all other covariates to 0), are called 'conditional effects.'\n\nIn order to create a scatter plot that displays th expression \\~ drug_dose_g relationship *corrected for mouse strain*, we would have to subtract the average effect of mouse strain on gene expression, from each data point depending on the strain of that animal.\n\nThis is called 'conditioning' or 'residualisation' and is beyond the scope of this course.\nJust be aware that a multiple linear regression model that finds very low p values for a main effect of interest, may be difficult to show graphically without 'residualising the data'.\n:::\n\n# Linear modelling in bioinformatics\n\nThe examples above illustrate a method of testing for associations in data using linear models, which works seamlessly with the same data formats required for ggplot.\n\nThe linear models used in bioinformatics research are no different in principal to those we have used above, however there are several computational and 'biotechnical' challenges that arise in bioinformatics analysis when using linear modeling.\n\nFirstly, its not uncommon to handle hundreds of thousands to millions of datapoints in a large bioinformatics analysis.\nDifferential expression analysis (of RNA protein or metabolite abundance), we will usually involve testing for treatment effects in 1000s of outcomes (e.g. individual genes), whilst correcting for multiple covariates, including batch effects.\n\nThis is a computationally expensive exercise that has been extensively optimized using matrix algebra in the Bioconductor packages developed at WEHI such as limma and edgeR, that we will learn about in [Chapter 6](chapter_6.qmd) & [Chapter 7](chapter_7.qmd).\n\nSecondly, the raw data distribution of biomolecular data is rarely normal, and requires extensive filtering, transformation, and measures to control 'mean-variance' relationships (heteroscedasticity) in the data.\nAgain, years of research has been invested in optimizing the data cleaning and normalization processes, and adapting linear modelling methods to sensitively detect true biological differences whilst minimizing false positives.\n\nLastly, to manage the computational and technical demands of linear modeling at scale, Bioconductor packages use specialized data 'containers' that hold expression data, sample and gene metadata, and linear model 'designs' together in a consistent format.\n\nThis supports a workflow that, unlike the interactive plotting/modelling workflow we have used to date in tidyverse, first loads all the required data and modelling parameters into a 'container', before fitting all of the models and generating thousands of results with a single command.\n\n# Machine learning\n\nMachine learning is an umbrella term for the use of statistics to 'learn' from features of a training dataset and then make predictions about a new dataset.\n\nMaking such predictions is a small addtional step from the linear modelling methods we introduced above.\n\nThe covariate effects (aka 'coefficients' or 'betas') output in a linear model summary table, can be combined with the `predict()` function to predict the outcome of a new dataset.\nIn other words, we could use `predict()` to solve the equation that will give us the expected gene expression (y) for a new mouse according to the drug dosage and strain of that individual.\n\nMachine learning is a huge field with many different algorithms that can learn relationships between features in training data.\n\nThe key difference between machine learning and the linear model-based hypothesis testing we introduced above, is that the former is primarily concerned with making accurate predictions about new data, whereas the latter is interested in understanding relationships between variables.\n\nWe will learn more about machine learning in [Chapter 8](chapter_8.qmd).\n",
    "supporting": [
      "stats_primer_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}